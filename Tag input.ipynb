{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('40_0.5_0.25_200_3_0.0105_Nadam.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import readfile, createBatches, createMatrices, iterate_minibatches, addCharInformation, padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charList =['(', 'q', '·', 'O', ')', '?', ':', 'Y', 'é', 'x', 'D', 'y', 'P', 'H', 'W', '9', 'I', 'm', 'l', 'N', 'o', '\"', 'J', '6', 'à', 't', 'Ñ', 'Q', 'ñ', 'K', 'n', '=', 'k', '3', '¡', '2', 'ú', 'R', 'p', 's', 'G', '1', 'M', 'g', 'z', '@', 'h', 'r', 'A', 'j', 'E', 'C', '0', '-', \"'\", 'e', 'w', '+', 'V', '7', 'v', 'u', 'B', 'a', '¿', '.', 'f', 'U', 'í', 'ó', '4', '%', 'á', 'S', 'd', 'c', '&', 'ü', '8', 'L', '!', '5', '/', 'X', 'T', 'Z', ';', ',', 'F', 'i', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateRealInput(object):\n",
    "    def __init__(self, INPUT, MODEL, charList):\n",
    "        self.input = nlp(INPUT)\n",
    "        self.Model = MODEL\n",
    "        self.charList = charList\n",
    "        \n",
    "    def load_data(self):\n",
    "        \n",
    "        sentences = []\n",
    "        sentence = []\n",
    "        for sent in self.input.sents:\n",
    "            #print(sent)\n",
    "            sentence = []\n",
    "            for token in sent:\n",
    "                #print(token)\n",
    "                if token.text == '\\n' or token.text == '\\n\\n'  :\n",
    "                    continue\n",
    "                else:\n",
    "                    sentence.append([token.text, ''])\n",
    "            sentences.append(sentence)\n",
    "        \"\"\"for token in self.input:\n",
    "            if len(token.text) ==0 or token.text == \"\\n\":\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            sentence.append([token.text.replace(' ', ''), ''])\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\"\"\"\n",
    "        \n",
    "        self.sentences = sentences\n",
    "    \n",
    "    def addCharInfo(self):\n",
    "        self.sentences = addCharInformation(self.sentences)\n",
    "        \n",
    "    def Embed(self):\n",
    "        labelSet = set()\n",
    "        words = {}\n",
    "        for sentence in self.sentences:\n",
    "            for token, char, label in sentence:\n",
    "                # token ... token, char ... list of chars, label ... BIO labels   \n",
    "                labelSet.add(label)\n",
    "                #charSet.add(char)\n",
    "                words[token.lower()] = True\n",
    "        \n",
    "        print(words)\n",
    "        self.label2Idx = {}\n",
    "        for label in labelSet:\n",
    "            self.label2Idx[label] = len(self.label2Idx)\n",
    "                \n",
    "        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n",
    "                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n",
    "        \n",
    "        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')\n",
    "        \n",
    "        word2Idx = {}\n",
    "        self.wordEmbeddings = []\n",
    "\n",
    "        fEmbeddings = open(\"./datasets/Embeddings/eswiki_20180420_100d.txt\", encoding=\"utf-8\")\n",
    "        fEmbeddings.readline()\n",
    "\n",
    "        # loop through each word in embeddings\n",
    "        for line in fEmbeddings:\n",
    "            split = line.strip().split(\" \")\n",
    "            word = split[0]  # embedding word entry\n",
    "\n",
    "            if len(word2Idx) == 0:  # add padding+unknown\n",
    "                word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "                vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
    "                self.wordEmbeddings.append(vector)\n",
    "\n",
    "                word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "                vector = np.random.uniform(-0.25, 0.25, len(split) - 1)\n",
    "                self.wordEmbeddings.append(vector)\n",
    "\n",
    "            if split[0].lower() in words:\n",
    "                vector = np.array([float(num) for num in split[1:]])\n",
    "                self.wordEmbeddings.append(vector)  # word embedding vector\n",
    "                word2Idx[split[0]] = len(word2Idx)  # corresponding word dict\n",
    "\n",
    "        self.wordEmbeddings = np.array(self.wordEmbeddings)\n",
    "        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
    "        #charList=list(charSet)\n",
    "        for c in self.charList:\n",
    "            self.char2Idx[c] = len(self.char2Idx)\n",
    "            \n",
    "        self.input_set = padding(createMatrices(self.sentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
    "        self.idx2Label = {v: k for k, v in self.label2Idx.items()}\n",
    "        self.idx2Word = {v: k for k, v in word2Idx.items()}\n",
    "    \n",
    "    def createBatches(self):\n",
    "        \"\"\"Create batches\"\"\"\n",
    "        self.input_batch, self.input_batch_len = createBatches(self.input_set)\n",
    "    \n",
    "    def tag_input(self):\n",
    "        \"\"\"Tag data with numerical values\"\"\"\n",
    "        correctLabels = []\n",
    "        predLabels = []\n",
    "        for i, data in enumerate(dataset):\n",
    "            tokens, casing, char, labels = self.input_set[i]\n",
    "            tokens = np.asarray([tokens])\n",
    "            casing = np.asarray([casing])\n",
    "            char = np.asarray([char])\n",
    "            pred = self.Model.predict([tokens, casing, char], verbose=False)[0]\n",
    "            pred = pred.argmax(axis=-1)  # Predict the classes\n",
    "            print(pred)\n",
    "        #return predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"La contracampaña electoral de Trump se basa en desarrollar una total operación de descrédito que cale entre los votantes y que ya fue utilizada contra Hillary Clinton en 2016, cuando se puso en duda la salud física de la aspirante demócrata a la Casa Blanca. Antiguos asesores de la ex secretaria de Estado ya han advertido al equipo de Biden que necesitan tomarse el asunto con la máxima seriedad. \"Biden no está respondiendo a las agresiones verbales y necesita hacerlo porque estos chismes calan en la opinión pública\", explica Philippe Reines, principal consejero de Clinton en 2016.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Sao Paulo (Brasil), 23 may (EFECOM).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK (\n",
      "OK )\n",
      "OK ,\n",
      "OK (\n",
      "OK )\n",
      "OK .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.is_punct:\n",
    "        print('OK', token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = EvaluateRealInput(string, model, charList)\n",
    "evaluate.load_data()\n",
    "evaluate.addCharInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.Embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.createBatches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, casing, char, labels = evaluate.input_set[0]\n",
    "tokens = np.asarray([tokens])\n",
    "casing = np.asarray([casing])\n",
    "char = np.asarray([char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = evaluate.Model.predict([tokens, casing, char], verbose=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('idx2label.pkl','gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.idx2Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label  = df.to_dict()['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(tokens, pred):\n",
    "    result = []\n",
    "    for i,token in enumerate(tokens[0]):\n",
    "        #print(i)\n",
    "        #print(idx2label)\n",
    "        result.append((evaluate.idx2Word[token], idx2label[pred[i]]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showResults(tokens, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string = \"HOLA MUNDO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenGetter(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fEmbeddings = open(\"./datasets/Embeddings/eswiki_20180420_100d.txt\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fEmbeddings.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1828809 100\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [array.split(\" \")[0] for array in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
